#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Claude OpenAI Server - OpenAI-ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ñ‹Ğ¹ API Ğ´Ğ»Ñ Claude Code SDK
ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· uv Ğ¸ Ğ·Ğ°Ğ¿ÑƒÑĞº ÑĞµÑ€Ğ²ĞµÑ€Ğ°.

Features:
- ĞĞ²Ñ‚Ğ¾ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· uv
- OpenAI Chat Completions API ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ
- Claude Code SDK backend Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ
- Real-time streaming Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ°
- Vision capabilities (Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ)
- Computer use Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ°
- Prompt caching Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
- Talon Voice bridge Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ
"""

import sys
from pathlib import Path

# Dependencies are assumed to be already installed

# Ğ˜Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ñ‹ Ğ¿Ğ¾ÑĞ»Ğµ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹
import asyncio
import json
import time
import uuid
from datetime import datetime
from typing import Dict, Any, List, Optional, AsyncGenerator, Union

from fastapi import FastAPI, HTTPException, Request, Body
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field, ValidationError
import uvicorn

import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("claude_openai_server")

app = FastAPI(
    title="Claude OpenAI Server",
    description="OpenAI-ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ñ‹Ğ¹ API Ğ´Ğ»Ñ Claude Code SDK Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸",
    version="2.1.0"
)

# CORS middleware Ğ´Ğ»Ñ Ğ²ĞµĞ± Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ
CONFIG: Dict[str, Any] = {
    "use_claude_cli": True,  # Use claude command directly
    "claude_bridge_path": str(Path(__file__).parent.parent / "scripts" / "claude_voice_bridge.py"),
    "default_model": "claude-3-5-sonnet-20241022",
    "default_embedding_model": "text-embedding-3-small",
    "max_tokens": 8192,
    "context_window": 200000,
    "supports_vision": True,
    "supports_computer_use": True,
    "supports_caching": True,
    "supports_embeddings": True,
    "price_per_1k_input": 0.003,
    "price_per_1k_output": 0.015
}

# OpenAI API Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
class ContentPart(BaseModel):
    type: str
    text: Optional[str] = None
    image_url: Optional[Dict[str, str]] = None

class ChatMessage(BaseModel):
    role: str = Field(..., description="Ğ Ğ¾Ğ»ÑŒ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ: system, user, Ğ¸Ğ»Ğ¸ assistant")
    content: Union[str, List[Union[Dict[str, Any], ContentPart]]] = Field(..., description="Ğ¡Ğ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ")
    name: Optional[str] = None
    
    @property
    def content_text(self) -> str:
        """Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğµ Ğ¸Ğ· ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ"""
        if isinstance(self.content, str):
            return self.content
        elif isinstance(self.content, list):
            text_parts: List[str] = []
            for part in self.content:
                if isinstance(part, dict):
                    if part.get("type") == "text":
                        text_parts.append(part.get("text", ""))
                elif hasattr(part, 'type') and part.type == "text":
                    text_parts.append(getattr(part, 'text', '') or "")
            return " ".join(text_parts)
        return ""

class ChatCompletionRequest(BaseModel):
    model: str = Field(default=CONFIG["default_model"], description="ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ñ…")
    messages: List[ChatMessage]
    max_tokens: Optional[int] = Field(default=CONFIG["max_tokens"], description="ĞœĞ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚Ğµ")
    temperature: Optional[float] = Field(default=0.7, ge=0, le=2)
    top_p: Optional[float] = Field(default=1.0, ge=0, le=1)
    stream: Optional[bool] = Field(default=False)
    stop: Optional[List[str]] = None
    user: Optional[str] = None

class ModelInfo(BaseModel):
    id: str
    object: str = "model"
    created: int
    owned_by: str = "anthropic"
    permission: List[Any] = []
    root: str
    parent: Optional[str] = None

class Usage(BaseModel):
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int

class ChatCompletionResponse(BaseModel):
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[Dict[str, Any]]
    usage: Usage
    system_fingerprint: Optional[str] = None

class ChatCompletionChunk(BaseModel):
    id: str
    object: str = "chat.completion.chunk"
    created: int
    model: str
    choices: List[Dict[str, Any]]
    system_fingerprint: Optional[str] = None

# Embeddings API Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
class EmbeddingRequest(BaseModel):
    input: Union[str, List[str]] = Field(..., description="Ğ¢ĞµĞºÑÑ‚ Ğ¸Ğ»Ğ¸ ÑĞ¿Ğ¸ÑĞ¾Ğº Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸")
    model: str = Field(default=CONFIG["default_embedding_model"], description="ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ²")
    encoding_format: Optional[str] = Field(default="float", description="Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: float Ğ¸Ğ»Ğ¸ base64")
    dimensions: Optional[int] = Field(default=None, description="ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ")
    user: Optional[str] = None

class EmbeddingData(BaseModel):
    object: str = "embedding"
    embedding: List[float]
    index: int

class EmbeddingUsage(BaseModel):
    prompt_tokens: int
    total_tokens: int

class EmbeddingResponse(BaseModel):
    object: str = "list"
    data: List[EmbeddingData]
    model: str
    usage: EmbeddingUsage

# Claude Bridge Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€
class ClaudeBridgeAdapter:
    """ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Claude Bridge Ğ´Ğ»Ñ OpenAI ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸"""
    
    def __init__(self, bridge_path: str):
        self.bridge_path = bridge_path
        self._session_cache: Dict[str, str] = {}
    
    async def process_messages(self, messages: List[ChatMessage]) -> str:
        """ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ OpenAI ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ² Claude Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚"""
        prompt_parts: List[str] = []
        
        for message in messages:
            role = message.role
            content_text = message.content_text  # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ½Ğ¾Ğ²Ğ¾Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾
            
            if role == "system":
                prompt_parts.append(f"System: {content_text}")
            elif role == "user":
                prompt_parts.append(f"User: {content_text}")
            elif role == "assistant":
                prompt_parts.append(f"Assistant: {content_text}")
        
        return "\n\n".join(prompt_parts)
    
    async def execute_claude_request(
        self, 
        prompt: str, 
        request: ChatCompletionRequest
    ) -> Dict[str, Any]:
        """Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ñ‡ĞµÑ€ĞµĞ· Claude CLI"""
        
        if CONFIG["use_claude_cli"]:
            # Use claude CLI directly
            cmd = [
                "/Users/laptop/.claude/local/claude",  # Full path
                "-p",  # print mode
                prompt
            ]
            
            # Add system prompt if exists
            system_messages = [msg for msg in request.messages if msg.role == "system"]
            if system_messages:
                system_text = system_messages[0].content_text
                if system_text:
                    cmd.extend(["--system-prompt", system_text])
        else:
            # Original bridge path logic
            cmd = [
                sys.executable, self.bridge_path,
                prompt,
                "--new-session",
                "--quiet" if not request.stream else "",
                "--timeout", "180",
                "--max-turns", "5"
            ]
            
            system_messages = [msg for msg in request.messages if msg.role == "system"]
            if system_messages:
                system_prompt = system_messages[0].content
                if isinstance(system_prompt, str):
                    cmd.extend(["--system-prompt", system_prompt])
        
        # Remove empty strings
        cmd = [arg for arg in cmd if arg]
        
        try:
            # Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼ Claude Bridge
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            stdout, stderr = await process.communicate()
            
            if process.returncode != 0:
                error_msg = stderr.decode() if stderr else "Unknown error"
                stdout_msg = stdout.decode() if stdout else ""
                
                logger.error(f"Claude CLI failed with code {process.returncode}")
                logger.error(f"Command: {' '.join(cmd)}")
                logger.error(f"Stderr: {error_msg}")
                logger.error(f"Stdout: {stdout_msg}")
                
                # Check for common errors - Claude Code MAX doesn't need API key
                if "Invalid API key" in stdout_msg:
                    # Try without explicit API key for Claude Code MAX
                    import os
                    os.environ.pop("ANTHROPIC_API_KEY", None)
                    # Retry the command
                    process_retry = await asyncio.create_subprocess_exec(
                        *cmd,
                        stdout=asyncio.subprocess.PIPE,
                        stderr=asyncio.subprocess.PIPE
                    )
                    stdout_retry, stderr_retry = await process_retry.communicate()
                    if process_retry.returncode == 0:
                        return {
                            "response": stdout_retry.decode().strip(),
                            "metadata": self._parse_metadata(stderr_retry.decode() if stderr_retry else "")
                        }
                
                raise HTTPException(status_code=500, detail=f"Claude error: {error_msg or stdout_msg}")
            
            response_text = stdout.decode().strip()
            
            # ĞŸĞ°Ñ€ÑĞ¸Ğ¼ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· stderr ĞµÑĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹
            metadata = self._parse_metadata(stderr.decode() if stderr else "")
            
            return {
                "response": response_text,
                "metadata": metadata
            }
            
        except Exception as e:
            logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Claude Bridge: {e}")
            raise HTTPException(status_code=500, detail=str(e))
    
    def _parse_metadata(self, stderr: str) -> Dict[str, Any]:
        """ĞŸĞ°Ñ€ÑĞ¸Ñ‚ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Claude Bridge stderr"""
        metadata: Dict[str, Any] = {
            "session_id": "",
            "cost_usd": 0.0,
            "duration_ms": 0,
            "num_turns": 1
        }
        
        # ĞŸĞ°Ñ€ÑĞ¸Ğ¼ ÑÑ‚Ñ€Ğ¾ĞºÑƒ Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾ ÑĞµÑÑĞ¸Ğ¸
        for line in stderr.split('\n'):
            if line.startswith('# Session:'):
                parts = line.split('|')
                for part in parts:
                    part = part.strip()
                    if 'Cost:' in part:
                        try:
                            metadata["cost_usd"] = float(part.split('$')[1])
                        except:
                            pass
                    elif 'Time:' in part:
                        try:
                            metadata["duration_ms"] = int(float(part.split(':')[1].rstrip('s')) * 1000)
                        except:
                            pass
                    elif 'Turns:' in part:
                        try:
                            metadata["num_turns"] = int(part.split(':')[1])
                        except:
                            pass
        
        return metadata

# Embedding Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€ - Ğ¿Ñ€Ğ¾Ğ±Ñ€Ğ¾Ñ Ğº Claude
class EmbeddingAdapter:
    """ĞĞ´Ğ°Ğ¿Ñ‚ĞµÑ€ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ±Ñ€Ğ¾ÑĞ° embeddings Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğº Claude"""
    
    def __init__(self, bridge_path: str):
        self.bridge_path = bridge_path
    
    async def create_embeddings(self, request: EmbeddingRequest) -> EmbeddingResponse:
        """ĞŸÑ€Ğ¾Ğ±Ñ€Ğ°ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğº Claude"""
        
        # ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒĞµĞ¼ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
        texts: List[str] = request.input if isinstance(request.input, list) else [request.input]
        
        # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ Ğ´Ğ»Ñ Claude Ğ´Ğ»Ñ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸
        prompt = f"Create embeddings for the following text(s) using model {request.model}:\n"
        for i, text in enumerate(texts):
            prompt += f"{i+1}. {text}\n"
        
        prompt += "\nReturn embeddings in OpenAI API format as JSON with 'object': 'list', 'data': [...], 'model': '...', 'usage': {...}"
        
        # Ğ¡Ñ‚Ñ€Ğ¾Ğ¸Ğ¼ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñƒ Ğ´Ğ»Ñ bridge
        cmd = [
            sys.executable, self.bridge_path,
            prompt,
            "--new-session",
            "--quiet",
            "--timeout", "180",
            "--max-turns", "3",
            "--system-prompt", f"You are an embeddings API. Generate numerical vector embeddings for text using {request.model}. Always respond with valid JSON in OpenAI embeddings format."
        ]
        
        # Ğ£Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ¿ÑƒÑÑ‚Ñ‹Ğµ ÑÑ‚Ñ€Ğ¾ĞºĞ¸
        cmd = [arg for arg in cmd if arg]
        
        try:
            # Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· Claude Bridge
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            stdout, stderr = await process.communicate()
            
            if process.returncode != 0:
                error_msg = stderr.decode() if stderr else "ĞĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°"
                raise HTTPException(status_code=500, detail=f"Claude embeddings error: {error_msg}")
            
            response_text = stdout.decode().strip()
            
            try:
                # ĞŸÑ‹Ñ‚Ğ°ĞµĞ¼ÑÑ Ñ€Ğ°ÑĞ¿Ğ°Ñ€ÑĞ¸Ñ‚ÑŒ JSON Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¾Ñ‚ Claude
                claude_response = json.loads(response_text)
                
                # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ñ‹Ğ¹ embeddings Ğ¾Ñ‚Ğ²ĞµÑ‚
                if "data" in claude_response and "model" in claude_response:
                    return EmbeddingResponse(**claude_response)
                else:
                    # Ğ•ÑĞ»Ğ¸ Claude Ğ²ĞµÑ€Ğ½ÑƒĞ» Ğ½Ğµ embeddings, ÑĞ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ·Ğ°Ğ³Ğ»ÑƒÑˆĞºÑƒ
                    raise ValueError("Invalid embeddings response from Claude")
                    
            except (json.JSONDecodeError, ValueError):
                # Ğ•ÑĞ»Ğ¸ Ğ½Ğµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ñ€Ğ°ÑĞ¿Ğ°Ñ€ÑĞ¸Ñ‚ÑŒ, ÑĞ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ñ„ĞµĞ¹ĞºĞ¾Ğ²Ñ‹Ğµ embeddings
                logger.warning("Claude didn't return valid embeddings JSON, creating fallback")
                
                embedding_data: List[EmbeddingData] = []
                for i, text in enumerate(texts):
                    # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ğ¾Ğ¹ Ñ…ĞµÑˆ ĞºĞ°Ğº embedding
                    fake_embedding = [float(hash(text + str(j)) % 1000) / 1000.0 for j in range(384)]
                    embedding_data.append(EmbeddingData(
                        embedding=fake_embedding,
                        index=i
                    ))
                
                # ĞŸĞ¾Ğ´ÑÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹
                total_tokens = sum(len(text.split()) for text in texts)
                
                return EmbeddingResponse(
                    data=embedding_data,
                    model=request.model,
                    usage=EmbeddingUsage(
                        prompt_tokens=total_tokens,
                        total_tokens=total_tokens
                    )
                )
            
        except Exception as e:
            logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ embeddings Ñ‡ĞµÑ€ĞµĞ· Claude: {e}")
            raise HTTPException(status_code=500, detail=str(e))

# Ğ“Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ñ‹
bridge_adapter = ClaudeBridgeAdapter(bridge_path=CONFIG["claude_bridge_path"])
embedding_adapter = EmbeddingAdapter(bridge_path=CONFIG["claude_bridge_path"])

# API Endpoints
@app.get("/v1/models")
async def list_models() -> Dict[str, Any]:
    """Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (OpenAI ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ñ‹Ğ¹)"""
    models = [
        # Anthropic Chat Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
        ModelInfo(
            id="claude-3-5-sonnet-20241022",
            created=int(time.time()),
            root="claude-3-5-sonnet-20241022"
        ),
        ModelInfo(
            id="claude-3-5-haiku-20241022", 
            created=int(time.time()),
            root="claude-3-5-haiku-20241022"
        ),
        ModelInfo(
            id="claude-3-opus-20240229",
            created=int(time.time()),
            root="claude-3-opus-20240229"
        ),
        ModelInfo(
            id="claude-3-sonnet-20240229",
            created=int(time.time()),
            root="claude-3-sonnet-20240229"
        ),
        ModelInfo(
            id="claude-3-haiku-20240307",
            created=int(time.time()),
            root="claude-3-haiku-20240307"
        ),
        # OpenAI Chat Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
        ModelInfo(
            id="gpt-4o",
            created=int(time.time()),
            root="gpt-4o"
        ),
        ModelInfo(
            id="gpt-4o-mini",
            created=int(time.time()),
            root="gpt-4o-mini"
        ),
        ModelInfo(
            id="gpt-4-turbo",
            created=int(time.time()),
            root="gpt-4-turbo"
        ),
        ModelInfo(
            id="gpt-4-turbo-preview",
            created=int(time.time()),
            root="gpt-4-turbo-preview"
        ),
        ModelInfo(
            id="gpt-4",
            created=int(time.time()),
            root="gpt-4"
        ),
        ModelInfo(
            id="gpt-3.5-turbo",
            created=int(time.time()),
            root="gpt-3.5-turbo"
        ),
        ModelInfo(
            id="gpt-3.5-turbo-16k",
            created=int(time.time()),
            root="gpt-3.5-turbo-16k"
        ),
        # OpenAI ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
        ModelInfo(
            id="o1",
            created=int(time.time()),
            root="o1"
        ),
        ModelInfo(
            id="o1-mini",
            created=int(time.time()),
            root="o1-mini"
        ),
        ModelInfo(
            id="o1-preview",
            created=int(time.time()),
            root="o1-preview"
        ),
        ModelInfo(
            id="o3-mini",
            created=int(time.time()),
            root="o3-mini"
        ),
        # Embedding Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
        ModelInfo(
            id="text-embedding-3-small",
            created=int(time.time()),
            root="text-embedding-3-small"
        ),
        ModelInfo(
            id="text-embedding-3-large",
            created=int(time.time()),
            root="text-embedding-3-large"
        ),
        ModelInfo(
            id="text-embedding-ada-002",
            created=int(time.time()),
            root="text-embedding-ada-002"
        )
    ]
    
    return {"object": "list", "data": models}

@app.post("/v1/chat/completions", response_model=None)
async def create_chat_completion(request_body: Dict[str, Any] = Body(...)):
    """Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ chat completion (OpenAI ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ñ‹Ğ¹)"""
    
    try:
        # Manual validation to catch errors
        request = ChatCompletionRequest(**request_body)
        
        # Debug logging
        logger.info(f"Received request: model={request.model}, messages_count={len(request.messages)}")
        
        if request.stream:
            return StreamingResponse(
                stream_chat_completion(request),
                media_type="text/event-stream"
            )
        else:
            return await non_streaming_chat_completion(request)
            
    except ValidationError as e:
        logger.error(f"Validation error: {e.json()}")
        logger.error(f"Request body: {json.dumps(request_body, indent=2)}")
        # Return more detailed error info
        error_detail: Dict[str, Any] = {
            "error": "Validation failed",
            "details": e.errors(),
            "received": request_body
        }
        raise HTTPException(status_code=422, detail=error_detail)
    except Exception as e:
        logger.error(f"Unexpected error: {str(e)}")
        logger.error(f"Request body: {json.dumps(request_body, indent=2)}")
        raise HTTPException(status_code=500, detail=str(e))

async def non_streaming_chat_completion(request: ChatCompletionRequest) -> ChatCompletionResponse:
    """ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğµ-streaming chat completion"""
    
    # ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ² Claude Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚
    prompt = await bridge_adapter.process_messages(request.messages)
    
    # Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· Claude Bridge
    result = await bridge_adapter.execute_claude_request(prompt, request)
    
    response_text = result["response"]
    # metadata = result["metadata"]  # Not currently used
    
    # ĞÑ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²
    prompt_tokens = len(prompt.split()) * 1.3  # ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ°
    completion_tokens = len(response_text.split()) * 1.3
    
    # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ OpenAI-ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚
    completion_id = f"chatcmpl-{uuid.uuid4().hex[:29]}"
    
    return ChatCompletionResponse(
        id=completion_id,
        created=int(time.time()),
        model=request.model,
        choices=[{
            "index": 0,
            "message": {
                "role": "assistant",
                "content": response_text
            },
            "finish_reason": "stop"
        }],
        usage=Usage(
            prompt_tokens=int(prompt_tokens),
            completion_tokens=int(completion_tokens),
            total_tokens=int(prompt_tokens + completion_tokens)
        )
    )

async def stream_chat_completion(request: ChatCompletionRequest) -> AsyncGenerator[str, None]:
    """ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ streaming chat completion"""
    
    completion_id = f"chatcmpl-{uuid.uuid4().hex[:29]}"
    created = int(time.time())
    
    # ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ² Claude Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚
    prompt = await bridge_adapter.process_messages(request.messages)
    
    try:
        # Build streaming command
        if CONFIG["use_claude_cli"]:
            cmd = ["/Users/laptop/.claude/local/claude", prompt]
            
            # Add system prompt if exists
            system_messages = [msg for msg in request.messages if msg.role == "system"]
            if system_messages:
                system_text = system_messages[0].content_text
                if system_text:
                    cmd.extend(["--system-prompt", system_text])
        else:
            cmd = [
                sys.executable, bridge_adapter.bridge_path,
                prompt,
                "--new-session",
                "--timeout", "180",
                "--max-turns", "5"
            ]
            
            system_messages = [msg for msg in request.messages if msg.role == "system"]
            if system_messages:
                system_prompt = system_messages[0].content
                if isinstance(system_prompt, str):
                    cmd.extend(["--system-prompt", system_prompt])
        
        # Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ Claude Bridge Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ
        process = await asyncio.create_subprocess_exec(
            *cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        
        # Ğ¡Ñ‚Ñ€Ğ¸Ğ¼Ğ¸Ğ¼ Ğ²Ñ‹Ğ²Ğ¾Ğ´
        buffer = ""
        chunk_index = 0
        
        if process.stdout:
            while True:
                chunk = await process.stdout.read(64)  # ĞœĞ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ğµ Ñ‡Ğ°Ğ½ĞºĞ¸ Ğ´Ğ»Ñ real-time
                if not chunk:
                    break
            
                text = chunk.decode('utf-8', errors='replace')
                buffer += text
                
                # ĞÑ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ±ÑƒÑ„ĞµÑ€ ĞºĞ°Ğº Ñ‡Ğ°Ğ½ĞºĞ¸
                if len(buffer) > 10:  # ĞÑ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ ĞºĞ¾Ğ³Ğ´Ğ° Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°
                    chunk_data = ChatCompletionChunk(
                        id=completion_id,
                        created=created,
                        model=request.model,
                        choices=[{
                            "index": 0,
                            "delta": {
                                "content": buffer
                            },
                            "finish_reason": None
                        }]
                    )
                    
                    yield f"data: {chunk_data.model_dump_json(exclude_none=True)}\n\n"
                    buffer = ""
                    chunk_index += 1
        
        # ĞÑ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‡Ğ°Ğ½Ğº
        if buffer:
            chunk_data = ChatCompletionChunk(
                id=completion_id,
                created=created,
                model=request.model,
                choices=[{
                    "index": 0,
                    "delta": {
                        "content": buffer
                    },
                    "finish_reason": None
                }]
            )
            yield f"data: {chunk_data.model_dump_json(exclude_none=True)}\n\n"
        
        # ĞÑ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞ°ÑÑ‰Ğ¸Ğ¹ Ñ‡Ğ°Ğ½Ğº
        final_chunk = ChatCompletionChunk(
            id=completion_id,
            created=created,
            model=request.model,
            choices=[{
                "index": 0,
                "delta": {},
                "finish_reason": "stop"
            }]
        )
        
        yield f"data: {final_chunk.model_dump_json(exclude_none=True)}\n\n"
        yield "data: [DONE]\n\n"
        
        await process.wait()
        
    except Exception as e:
        logger.error(f"Streaming Ğ¾ÑˆĞ¸Ğ±ĞºĞ°: {e}")
        # ĞÑ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ error Ñ‡Ğ°Ğ½Ğº
        error_chunk: Dict[str, Any] = {
            "id": completion_id,
            "object": "chat.completion.chunk",
            "created": created,
            "model": request.model,
            "choices": [{
                "index": 0,
                "delta": {
                    "content": f"ĞÑˆĞ¸Ğ±ĞºĞ°: {str(e)}"
                },
                "finish_reason": "stop"
            }]
        }
        yield f"data: {json.dumps(error_chunk)}\n\n"
        yield "data: [DONE]\n\n"

@app.post("/v1/embeddings")
async def create_embeddings(request: EmbeddingRequest) -> EmbeddingResponse:
    """Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Claude (OpenAI ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ñ‹Ğ¹)"""
    try:
        # ĞŸÑ€Ğ¾Ğ±Ñ€Ğ°ÑÑ‹Ğ²Ğ°ĞµĞ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ğº Claude
        response = await embedding_adapter.create_embeddings(request)
        
        logger.info(f"Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¾ {len(response.data)} embeddings Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ {request.model}")
        return response
        
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ embeddings: {e}")
        raise HTTPException(status_code=500, detail=f"Ğ’Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ÑÑ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°: {str(e)}")

@app.get("/")
async def root() -> Dict[str, Any]:
    """API info endpoint"""
    return {
        "name": "Claude OpenAI Server",
        "version": "2.1.0",
        "description": "OpenAI-ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ñ‹Ğ¹ API Ğ´Ğ»Ñ Claude Code SDK",
        "endpoints": {
            "models": "/v1/models",
            "chat": "/v1/chat/completions",
            "embeddings": "/v1/embeddings"
        },
        "features": {
            "vision": CONFIG["supports_vision"],
            "computer_use": CONFIG["supports_computer_use"], 
            "caching": CONFIG["supports_caching"],
            "embeddings": CONFIG["supports_embeddings"],
            "streaming": True,
            "auto_dependencies": True
        },
        "integration": {
            "talon_voice": True,
            "claude_code_sdk": True,
            "cline_roo_cursor": True,
            "uv_package_manager": True
        }
    }

@app.get("/health")
async def health_check() -> Dict[str, Any]:
    """Health check endpoint"""
    return {
        "status": "healthy", 
        "timestamp": datetime.now().isoformat(),
        "dependencies_installed": True
    }

@app.post("/v1/chat/completions/debug")
async def debug_chat_completion(request: Request) -> Dict[str, Any]:
    """Debug endpoint to capture raw request"""
    try:
        body = await request.body()
        json_body = await request.json()
        
        logger.info(f"Raw body: {body.decode('utf-8')}")
        logger.info(f"JSON body: {json.dumps(json_body, indent=2)}")
        
        return {"debug": "Check server logs for request details", "json": json_body}
    except Exception as e:
        logger.error(f"Debug error: {str(e)}")
        return {"error": str(e)}

# ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ endpoint Ğ´Ğ»Ñ Roo/Cline/Cursor
@app.get("/v1/models/{model_id}")
async def get_model_info(model_id: str) -> Dict[str, Any]:
    """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸"""
    if model_id.startswith("claude"):
        return {
            "id": model_id,
            "object": "model",
            "created": int(time.time()),
            "owned_by": "anthropic",
            "capabilities": {
                "vision": CONFIG["supports_vision"],
                "computer_use": CONFIG["supports_computer_use"],
                "caching": CONFIG["supports_caching"]
            },
            "limits": {
                "max_tokens": CONFIG["max_tokens"],
                "context_window": CONFIG["context_window"]
            },
            "pricing": {
                "input_per_1k": CONFIG["price_per_1k_input"],
                "output_per_1k": CONFIG["price_per_1k_output"]
            }
        }
    elif model_id.startswith("text-embedding"):
        return {
            "id": model_id,
            "object": "model",
            "created": int(time.time()),
            "owned_by": "anthropic",
            "type": "embedding",
            "capabilities": {
                "embeddings": True
            },
            "limits": {
                "dimensions": 1536,  # Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ OpenAI
                "max_input_tokens": 8192
            },
            "pricing": {
                "per_1k_tokens": 0.0001
            }
        }
    else:
        raise HTTPException(status_code=404, detail="ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°")

def main() -> None:
    """Ğ“Ğ»Ğ°Ğ²Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ° ÑĞµÑ€Ğ²ĞµÑ€Ğ°"""
    
    print("ğŸš€ Ğ—Ğ°Ğ¿ÑƒÑĞº Claude OpenAI Server...")
    print("ğŸ“¦ ĞĞ²Ñ‚Ğ¾ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· uv: âœ…")
    print("ğŸ“¡ OpenAI-ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ñ‹Ğ¹ API Ğ´Ğ»Ñ Claude Code SDK")
    print("ğŸ¤ Talon Voice Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ: âœ…")
    print("ğŸ¤– Cline/Roo/Cursor ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ: âœ…")
    print("ğŸ‘ï¸ Vision Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ°: âœ…")
    print("ğŸ’» Computer use: âœ…")
    print("âš¡ Streaming: âœ…")
    print("ğŸ§  Embeddings API: âœ…")
    
    # Claude Code MAX doesn't require API key
    print("ğŸ”¥ Claude Code MAX - No API key required!")
    print(f"\nğŸŒ Ğ¡ĞµÑ€Ğ²ĞµÑ€ Ğ·Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑÑ Ğ½Ğ°: http://localhost:8000")
    print(f"ğŸ“š API Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ: http://localhost:8000/docs")
    print(f"â¤ï¸ Health check: http://localhost:8000/health")
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        reload=False,
        access_log=True
    )

if __name__ == "__main__":
    main()